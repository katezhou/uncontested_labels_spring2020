{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy.random import choice\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import scipy.stats as stats\n",
    "from IPython.display import display, HTML\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "from statistics import mode\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    def __init__(self, data, label_name, name):\n",
    "        self.data = data\n",
    "        #The column name for the label in the dataset\n",
    "        self.label_name = label_name\n",
    "        #name of dataset\n",
    "        self.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trinarize data to be consistent to 2 or 3 categories\n",
    "def trinarize_misinfo(input):\n",
    "    if (input == \"Very high credibility\" or input == \"Somewhat high credibility\"):\n",
    "        return 1\n",
    "    elif (input == \"Medium credibility\"):\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trinarize_snli(input):\n",
    "    if(input == 'entailment'):\n",
    "        return 1\n",
    "    elif(input == 'neutral'):\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trinarize_sentiment(input):\n",
    "    if(int(input) > 0):\n",
    "        return 1\n",
    "    elif(int(input) == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data and grouping annotations by tasks\n",
    "def get_data(data_file):\n",
    "    if (data_file == 'toxicity'):\n",
    "        data = pd.read_csv(\"data/TOXICITY_toxicity_individual_annotations.csv\").head(10000)\n",
    "        grouped = data.groupby(\"id\")[['id', 'toxic']]\n",
    "    \n",
    "    if (data_file == 'misinfo'):\n",
    "        data = pd.read_csv(\"data/MISINFO_misinfo_credco_2019_study_cplusj_2020_subset.csv\")\n",
    "        annotators = pd.read_csv(\"data/MISINFO_misinfo_credco_study_2019_crowd_annotators_simple.csv\")\n",
    "        \n",
    "        #combine data with annotators\n",
    "        data = data[data['annotator'].isin(annotators[annotators['pool']=='Upwork']['annotator_id'].values)]\n",
    "        \n",
    "        #one annotator that had 26 labels insteaad of 25\n",
    "        data = data[data['annotator'] != 'CredCo-3AA.33']\n",
    "        data = data[['report_title', 'task_1_answer']]\n",
    "        data['task_1_answer'] = [trinarize_misinfo(x) for x in data['task_1_answer']]\n",
    "        grouped = data.groupby(\"report_title\")\n",
    "    \n",
    "    if (data_file == 'pg13'):\n",
    "        data = pd.read_csv(\"data/PG13_labels.txt\", sep='\\t', header=None)\n",
    "        #keep only annotations with at least ten labels\n",
    "        filtered = data.groupby(1)[1].filter(lambda x: len(x) >= 10)\n",
    "        grouped = data[data[1].isin(filtered)].groupby(1)\n",
    "    \n",
    "    if (data_file == 'bots'):\n",
    "        data = pd.read_csv(\"data/BOTS_crowdflower_results_detailed.csv\")[['crowdflower_id', 'class']]\n",
    "        #keep only annotations with at least three labels\n",
    "        filtered = data.groupby('crowdflower_id')['crowdflower_id'].filter(lambda x: len(x) >= 3)\n",
    "        grouped = data[data['crowdflower_id'].isin(filtered)].groupby('crowdflower_id')\n",
    "\n",
    "    if (data_file == 'snli'):\n",
    "        # snli check\n",
    "        # dynamic worker routing\n",
    "        data = pd.read_json(\"data/SNLI_snli_1.0_train.jsonl\", lines=True)\n",
    "        labels = []\n",
    "        for name, row in data.iterrows():\n",
    "        #keep only annotations with at least five labels\n",
    "            if len(row[\"annotator_labels\"]) >= 5:\n",
    "                for label in row[\"annotator_labels\"]:\n",
    "                    labels.append({\"idx\": name, \"label\": label})\n",
    "        data = pd.DataFrame(labels)\n",
    "        data['label'] = [trinarize_snli(x) for x in data[['label']].values]\n",
    "        data = data[['idx', 'label']]\n",
    "        grouped = data.groupby(\"idx\")\n",
    "        \n",
    "    if (data_file == 'sentiment'):\n",
    "        data = pd.read_table(\"data/SENTIMENT_cred_event_TurkRatings.data\", sep='\\t')\n",
    "        labels = []\n",
    "        for name, row in data.iterrows():\n",
    "            #keep only annotations with at least thirty labels\n",
    "            if len(row[\"Cred_Ratings\"]) >= 30:\n",
    "                row = row['Cred_Ratings'][1:-1].split(\",\")\n",
    "                for label in row:\n",
    "                    labels.append({\"idx\": name, \"label\": label})\n",
    "        data = pd.DataFrame(labels)\n",
    "        data['label'] = [trinarize_sentiment(int(x[0].strip(\"\\', \\\"\"))) for x in data[['label']].values]\n",
    "        data = data[['idx', 'label']]\n",
    "        grouped = data.groupby(\"idx\")\n",
    "    \n",
    "    if (data_file == 'wsd'):\n",
    "        data = pd.read_csv(\"data/WSD_wsd.standardized.tsv\", sep='\\t')\n",
    "        grouped = data.groupby(\"orig_id\")\n",
    "        \n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find a way to generate samples and finding the majority from each sample\n",
    "def find_majority(values, sample_size):\n",
    "    random = choice(values, sample_size * 2, replace=True)\n",
    "    sample1 = Counter(random[:sample_size]).most_common(sample_size)\n",
    "    sample2 = Counter(random[sample_size:]).most_common(sample_size)\n",
    "    majority_1 = sample1[0][0]\n",
    "    majority_2 = sample2[0][0]  \n",
    "    if ((len(sample1) > 1 and sample1[0][1] == sample1[1][1])): #no majority\n",
    "        majority_1 = 'NM'\n",
    "    if (len(sample2) > 1 and sample2[0][1] == sample2[1][1]): #no majority\n",
    "        majority_2 = 'NM'\n",
    "    return sample1, sample2, majority_1, majority_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create parellel datasets of size 1, 3, 5, 7, 9\n",
    "#and compare flip rate between the parallel datasets\n",
    "#Resample 100 times and take the average of these iterations\n",
    "def resample_flipping(data_object):\n",
    "    for sample_size in [1, 3, 5, 7, 9]:\n",
    "        flip_count = 0\n",
    "        for iterations in tqdm(range(0, 100)):\n",
    "            for name, group in data_object.data:\n",
    "                sample1, sample2, majority_1, majority_2 = find_majority(group[data_object.label_name].values, sample_size)\n",
    "                if (majority_1 != majority_2):\n",
    "                    flip_count += 1\n",
    "\n",
    "        print('sample size', sample_size)\n",
    "        print(\"flip average percent\", round(flip_count/len(data_object.data), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a parallel dataset, how many of these labels flip?\n",
    "#Create another parallel dataset, how many of original labels flipped at least once?\n",
    "#Repeat this process for up to 5 parallel datasets\n",
    "#Repeat this process 100 times and take the average number of flips for 1, 2, 3, 4, 5 parallele datasets\n",
    "def population_level(data_object):\n",
    "    sample_size = 5\n",
    "    percent_map = {}\n",
    "    for x in tqdm(range(0, 100)):\n",
    "        flipped_labels = {}\n",
    "        label_count = {}\n",
    "\n",
    "        for iteration in range(1, 6):\n",
    "            temp = []\n",
    "            for name, group in data_object.data:\n",
    "                values = group[data_object.label_name].values\n",
    "                #find the gold label for the data point\n",
    "                actual_label = Counter(values).most_common(len(values))[0][0]\n",
    "                #keep track of how many gold labels there are\n",
    "\n",
    "                if (actual_label not in label_count):\n",
    "                    label_count[actual_label] = set()\n",
    "                    flipped_labels[actual_label] = set()\n",
    "                \n",
    "                label_count[actual_label].add(name) \n",
    "                #keep track of how many labels in this category flipped\n",
    "                sample1, sample2, majority_1, majority_2 = find_majority(group[data_object.label_name].values, sample_size)\n",
    "                if (majority_1 != majority_2):\n",
    "                    flipped_labels[actual_label].add(name)\n",
    "\n",
    "            for actual_label in label_count:\n",
    "                percent = round(len(flipped_labels[actual_label])/len(label_count[actual_label]), 2)\n",
    "                key = \"iteration: \" + str(iteration) + \" label:\" + str(actual_label)\n",
    "                percent_map[key] = round(percent_map.get(key, 0) + percent, 2)\n",
    "    for x in percent_map:\n",
    "        print(x, percent_map[x])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess data for each of the data to calculate\n",
    "#fleiss kappa scores\n",
    "def calculate_fleiss_kappa(data_object):\n",
    "    result = []\n",
    "    if (data_object.name == 'pg13'):        \n",
    "        map = {'G':0, 'P':1, 'X':2, 'R':3}\n",
    "        for i, x in data_object.data:\n",
    "            #need at least five samples\n",
    "            #create a sample out of 5\n",
    "            if (len(list(x[data_object.label_name].values)) >= 5):\n",
    "                counts = Counter(random.sample(list(x[data_object.label_name].values), 5))\n",
    "                temp = [0] * 4\n",
    "                for y in counts:\n",
    "                    temp[map[y]] = counts[y]\n",
    "                result.append(temp)\n",
    "            \n",
    "    if (data_object.name == 'misinfo' or data_object.name == 'snli'):        \n",
    "        for i, x in data_object.data:\n",
    "            counts = Counter(x[data_object.label_name].values)\n",
    "            temp = [0] * 3\n",
    "            for y in counts:\n",
    "                temp[y + 1] = counts[y]\n",
    "            result.append(temp)\n",
    "    \n",
    "    if (data_object.name == 'wsd'):        \n",
    "        for i, x in data_object.data:\n",
    "            counts = Counter(x[data_object.label_name].values)\n",
    "            temp = [0] * 3\n",
    "            for y in counts:\n",
    "                temp[y - 1] = counts[y]\n",
    "            result.append(temp)\n",
    "    \n",
    "    if (data_object.name == 'sentiment'):\n",
    "        for i, x in data_object.data:\n",
    "            if (len(list(x[data_object.label_name].values)) >= 30):\n",
    "                counts = Counter(x[data_object.label_name].values)\n",
    "                temp = [0] * 3\n",
    "                for y in counts:\n",
    "                    temp[y + 1] = counts[y]\n",
    "                result.append(temp)\n",
    "    \n",
    "    if (data_object.name =='toxic'):\n",
    "        for name, group in tqdm(data_object.data):\n",
    "            subsample = group.sample(5, replace = True)\n",
    "            temp = subsample['toxic'].values\n",
    "            result.append(temp)\n",
    "\n",
    "        result = pd.DataFrame(result)\n",
    "        result['toxic'] = result.sum(axis=1)\n",
    "        result['non_toxic'] = 5 - result['toxic']\n",
    "        result = result[['toxic', 'non_toxic']].values\n",
    "    \n",
    "    if (data_object.name == 'bots'):        \n",
    "        map = {'genuine':0, 'spambot':1, 'unable':2}\n",
    "        for i, x in data_object.data:\n",
    "            if (len(list(x[data_object.label_name].values)) >= 3):\n",
    "                counts = Counter(x[data_object.label_name].values)\n",
    "                temp = [0] * 3\n",
    "                for y in counts:\n",
    "                    temp[map[y]] = counts[y]\n",
    "                result.append(temp)\n",
    "    print(\"fleiss kappa score: \", fleiss_kappa(result, method='fleiss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create parallel datasets\n",
    "def calculate_class_difference_helper(data_object):\n",
    "    sample_size_to_resampled_dataset = {}\n",
    "\n",
    "    for sample_size in [5, 7, 9]:\n",
    "        resampled_datasets = [[], [],[], []]\n",
    "        for iterations in tqdm(range(0, 100)):\n",
    "            for name, group in data_object.data:\n",
    "                sample1, sample2, majority_1, majority_2 = find_majority(group[data_object.label_name].values, sample_size)\n",
    "\n",
    "                resampled_datasets[0].append(name)\n",
    "                resampled_datasets[1].append(majority_1)\n",
    "                resampled_datasets[2].append(majority_2)\n",
    "                to_append = 0 # assume no difference\n",
    "\n",
    "                if (majority_1 != majority_2):\n",
    "                    # in case sample2 didn't have any votes for the majority class from sample1\n",
    "                    to_append= abs(sample1[0][1])\n",
    "                    for x in sample2:\n",
    "                        if (x[0] == majority_1):\n",
    "                            to_append = abs(sample1[0][1] - x[1])\n",
    "                resampled_datasets[3].append(to_append)\n",
    "        sample_size_to_resampled_dataset[sample_size] = resampled_datasets\n",
    "    return sample_size_to_resampled_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#across the parallel datasets, calculate how large of a difference exists\n",
    "#between contested labels\n",
    "def calculate_difference_of_flip(sample_size_to_resampled_dataset):\n",
    "    for sample_size in sample_size_to_resampled_dataset:\n",
    "        print(\"sample size\", sample_size)\n",
    "        resampled_dataset = sample_size_to_resampled_dataset[sample_size]\n",
    "        results = pd.DataFrame(resampled_dataset).transpose()\n",
    "        \n",
    "        #get rid of no modes\n",
    "        results = results[results[1] != 'NM']\n",
    "        results = results[results[2] != 'NM']\n",
    "        results[3] = pd.to_numeric(results[3])\n",
    "        \n",
    "        print(\"mean\")\n",
    "        print(round(results[results[1] != results[2]][[3]].mean().values[0], 2))\n",
    "        print(\"std\")\n",
    "        print(round(results[results[1] != results[2]][[3]].std().values[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Across the parallel datasets, calculate the flip rate for each class\n",
    "#given sample size of 5\n",
    "def calculate_flips_per_class(sample_size_to_resampled_dataset):\n",
    "    resampled_dataset = sample_size_to_resampled_dataset[5]\n",
    "    resampled_dataset = pd.DataFrame(resampled_dataset).transpose()\n",
    "    \n",
    "    grouped = resampled_dataset[resampled_dataset[1] != resampled_dataset[2]].groupby([1]).count()\n",
    "    print(\"flip per class\")\n",
    "#     display(grouped)\n",
    "    print(\"total count\")\n",
    "    counts = resampled_dataset.groupby([1]).count()\n",
    "#     display(counts)\n",
    "    print(round(grouped[0]/counts[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run any of the tests\n",
    "def run_test(datasets, test, helper=False):\n",
    "    for dataset in datasets:\n",
    "        print('------------------------')\n",
    "        print(dataset.name)\n",
    "        if (helper):\n",
    "            sample_size_to_resampled_dataset = calculate_class_difference_helper(dataset)\n",
    "            calculate_difference_of_flip(sample_size_to_resampled_dataset)\n",
    "            calculate_flips_per_class(sample_size_to_resampled_dataset)\n",
    "        else:\n",
    "            test(dataset)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create all the data objects\n",
    "toxicity_object = dataset(get_data(\"toxicity\"), 'toxic', 'toxic')\n",
    "# misinfo_object = dataset(get_data(\"misinfo\"), 'task_1_answer', 'misinfo')\n",
    "# pg13_object = dataset(get_data(\"pg13\"), 2, 'pg13')\n",
    "# bots_object =  dataset(get_data(\"bots\"), 'class', 'bots')\n",
    "# snli_object = dataset(get_data(\"snli\"), 'label', 'snli')\n",
    "# sentiment_object = dataset(get_data(\"sentiment\"), 'label', 'sentiment')\n",
    "# wsd_object = dataset(get_data(\"wsd\"), 'response', 'wsd')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "toxic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/katezhou/lib/python3.6/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75126d02118c44c08734087d6de580c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec180b548b1472c94fd5dc9d7600e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e586fcc13e4cdaa6afae97fd71171c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sample size 5\n",
      "mean\n",
      "1.99\n",
      "std\n",
      "0.85\n",
      "sample size 7\n",
      "mean\n",
      "2.33\n",
      "std\n",
      "1.07\n",
      "sample size 9\n",
      "mean\n",
      "2.66\n",
      "std\n",
      "1.24\n",
      "flip per class\n",
      "total count\n",
      "1\n",
      "0    0.03\n",
      "1    0.29\n",
      "Name: 0, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_of_datasets = [toxicity_object]\n",
    "#, misinfo_object, pg13_object, bots_object, snli_object, sentiment_object, wsd_object\n",
    "#test 1 - Resample Flip\n",
    "# run_test(list_of_datasets, resample_data)\n",
    "# #test 2 - Population Level\n",
    "# run_test(list_of_datasets, population_level)\n",
    "# #test 3 - Fleiss Kappa\n",
    "# run_test(list_of_datasets, calculate_fleiss_kappa)\n",
    "# Helper + test 4, test 5 - Difference in Flip, Flips across Classes\n",
    "run_test(list_of_datasets, None, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
